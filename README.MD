# üõí Retail Data Pipeline using Airflow and AWS

Este proyecto implementa un pipeline de ingenier√≠a de datos para centralizar, procesar y disponibilizar informaci√≥n proveniente de m√∫ltiples fuentes heterog√©neas (MySQL, Azure Data Lake Storage (ADLS) y MongoDB) sobre un data lake en AWS, demostrando habilidades en orquestaci√≥n, transformaci√≥n de datos y an√°lisis eficiente en la nube.

## üß† Descripci√≥n General

El pipeline permite la ingesta de datos crudos desde distintas fuentes hacia un bucket de Amazon S3 (Landing), su posterior transformaci√≥n y enriquecimiento mediante Python y Airflow en Docker, y la disponibilidad de los datos procesados para consultas SQL con Amazon Athena.

Se generan datasets enriquecidos y KPI calculados, almacenados en la capa Processed en formato Parquet, optimizando el rendimiento y costos de an√°lisis. Los datos procesados son catalogados con AWS Glue, permitiendo exploraci√≥n y an√°lisis de manera eficiente.

## üèóÔ∏è Arquitectura del Proyecto

El flujo de trabajo combina servicios en la nube y procesamiento local:

1.  **Extracci√≥n e Ingesta** 
    -   Scripts en Python leen datos de MySQL, ADLS y MongoDB.       
    -   Se escriben datos crudos en **S3 Landing**.
        
2.  **Transformaci√≥n y Enriquecimiento**
    -   Scripts de transformaci√≥n y enriquecimiento en Python.     
    -   Airflow orquesta los DAGs asegurando ejecuci√≥n secuencial y autom√°tica.
        
3.  **Capa Procesada**
    -   Datos transformados se escriben en **S3 Processed** en **Parquet**.
    -   Se generan subcarpetas de `enriched` y `kpis`.
        
4.  **Catalogaci√≥n y Consulta**
    -   AWS Glue registra las tablas de datos procesados.
    -   Amazon Athena permite consultas SQL y an√°lisis exploratorio.
  
## üß∞ Tecnolog√≠as y Herramientas

-   Lenguaje: Python 3.x
-   Orquestador: Apache Airflow
-   Contenedores: Docker
-   Almacenamiento y an√°lisis: Amazon S3, AWS Glue, Amazon Athena
-   Fuentes de datos: MySQL, ADLS, MongoDB
-   Formato optimizado: Parquet

## üìÅ Estructura del Repositorio

```text
retail-multisource-pipeline-python-airflow-aws/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ etl_pipeline_dag.py
‚îú‚îÄ‚îÄ diagrams/
‚îÇ   ‚îî‚îÄ‚îÄ arquitectura_retail_multisource_pipeline.png
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ enriched.py
‚îÇ   ‚îú‚îÄ‚îÄ read.py
‚îÇ   ‚îî‚îÄ‚îÄ upload.py
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ data_enriched_kpis.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ data_load.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ data_read.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ data_seeding.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ data_transform.ipynb
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py
‚îÇ   ‚îú‚îÄ‚îÄ load.py
‚îÇ   ‚îî‚îÄ‚îÄ transform.py
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ connections.py
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .env-ejemplo
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ requirements.txt
```

## ‚òÅÔ∏è Estructura del Data Lake en S3

```text
s3://retail-multisource-pipeline/
‚îú‚îÄ‚îÄ landing/                  # Datos crudos sin transformar
‚îú‚îÄ‚îÄ processed/                # Datos transformados
‚îÇ   ‚îú‚îÄ‚îÄ enriched/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enriched_categories/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enriched_categories.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enriched_customers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enriched_customers.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enriched_products/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enriched_products.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enriched_order_items/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enriched_order_items.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enriched_orders/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ enriched_orders.parquet
‚îÇ   ‚îÇ    
‚îÇ   ‚îî‚îÄ‚îÄ kpis/
‚îÇ       ‚îú‚îÄ‚îÄ kpi_ventas_mes/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ kpi_ventas_mes.parquet
‚îÇ       ‚îú‚îÄ‚îÄ kpi_top_productos/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ kpi_top_productos.parquet
‚îÇ       ‚îú‚îÄ‚îÄ kpi_ticket_promedio/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ kpi_ticket_promedio.parquet
‚îÇ       ‚îú‚îÄ‚îÄ kpi_clientes_recurrentes/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ kpi_clientes_recurrentes.parquet
‚îÇ       ‚îî‚îÄ‚îÄ kpi_ventas_estado/
‚îÇ           ‚îî‚îÄ‚îÄ kpi_ventas_estado.parquet
‚îî‚îÄ‚îÄ athena-query-results/     # Resultados temporales de queries
```

## ‚öôÔ∏è Requisitos
-   Docker >= 20.x
-   Docker Compose >= 1.29.x  
-   Python >= 3.9 (solo si deseas ejecutar scripts localmente)    
-   Acceso a AWS con permisos para S3, Glue y Athena

## ‚öôÔ∏è Instrucciones de Uso

1.  Configura las credenciales de AWS con permisos para S3, Glue y Athena.

3.  Crea el bucket principal `retail-multisource-pipeline` en S3 y aseg√∫rate de que tenga las carpetas `landing` y `processed`.

4. Haz el poblado de datos del archivo `retail_db` en cada una de las fuentes externas (**MySQL, ADLS, MongoDB**).

5.  Clona el repositorio y navega al directorio del proyecto:
   `git clone https://github.com/LemuelAzael/retail-multisource-pipeline-python-airflow-aws.git cd retail-multisource-pipeline-python-airflow-aws` 

6.  Construye los contenedores Docker:
`docker-compose build` 

7.  Levanta los servicios:
    `docker-compose up -d` 

8.  Accede a **Airflow UI** en tu navegador:
    `http://localhost:8080` 

9.  Ejecuta los DAGs para que se realice la ingesta, transformaci√≥n y carga de los datos.
    
10.  Verifica que los datasets procesados se encuentren en **Processed**, organizados en las carpetas `enriched` y `kpis`.
    
11.  Consulta los datos mediante **Amazon Athena**, usando las tablas registradas en **Glue Data Catalog**.


## üîÑ Pipeline de Datos

| Etapa               | Servicio / Script              | Descripci√≥n                                                  |
| ------------------- | ------------------------------ | ------------------------------------------------------------ |
| Fuentes de datos    | MySQL, ADLS, MongoDB           | Contienen datos crudos de retail.                            |
| Ingesta             | scripts/ingest.py + Landing S3 | Copia los datos originales al bucket de landing.             |
| Transformaci√≥n      | scripts/transform.py           | Limpieza, transformaci√≥n y enriquecimiento de datos.         |
| Procesamiento       | Airflow DAGs                   | Coordina ejecuci√≥n secuencial de scripts.                    |
| Almacenamiento      | S3 Processed                   | Guarda datos transformados en Parquet (`enriched` y `kpis`). |
| Catalogaci√≥n        | AWS Glue Data Catalog          | Registra los datos procesados como tablas para consultas.    |
| Consulta y an√°lisis | Amazon Athena                  | Permite consultas SQL y an√°lisis exploratorio.               |


## üéØ Objetivos del Proyecto
-   Integrar y centralizar datos de m√∫ltiples fuentes heterog√©neas.    
-   Dise√±ar pipelines automatizados, reproducibles y escalables.    
-   Transformar, limpiar y enriquecer datos de forma eficiente.    
-   Catalogar y disponibilizar datos para an√°lisis SQL en la nube.    
-   Aplicar buenas pr√°cticas de ingenier√≠a de datos en un entorno profesional.

## üë§ Autor
Lemuel Azael Carrillo Barrera  
Proyecto para portafolio de ingenier√≠a de datos.

üîó [Sobre m√≠](https://www.notion.so/Perfil-profesional-Lemuel-Azael-Carrillo-Barrera-2619ec6ab8528025b300d6099cd92add?source=copy_link)

## üìö Desarrollo completo del proyecto

Revisa todo el proceso paso a paso: desde la configuraci√≥n inicial hasta la ejecuci√≥n automatizada en AWS.

üîé [Ver desarrollo completo en Notion](https://www.notion.so/Data-Pipeline-for-Retail-using-Airflow-and-AWS-2619ec6ab8528051a6d2c7d1dbcf24ed?source=copy_link)

## üåê Portafolio completo

Explora m√°s proyectos y recursos en mis portafolios personales:

üîó [GitHub ‚Äì Portafolio](https://github.com/LemuelAzael/LemuelAzael)
üîó [Notion ‚Äì Portafolio](https://www.notion.so/Portafolio-Lemuel-Carrillo-2179ec6ab8528029ba54f3bf3363f993?source=copy_link)
